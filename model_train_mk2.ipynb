{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "950d3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFKC\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "# 1. 초기 토크나이저 구성\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = NFKC()\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 2. 학습자 정의\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=3000,\n",
    "    special_tokens=[\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"]\n",
    ")\n",
    "\n",
    "# 3. 학습 실행\n",
    "tokenizer.train([\"./create-dataset/dataset/train.txt\"], trainer)\n",
    "\n",
    "# 4. 후처리 설정 (BOS, EOS)\n",
    "tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[BOS] $A [EOS]\",\n",
    "    pair=\"[BOS] $A [EOS] $B:1 [EOS]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", tokenizer.token_to_id(\"[BOS]\")),\n",
    "        (\"[EOS]\", tokenizer.token_to_id(\"[EOS]\")),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 5. 저장 (Hugging Face 호환 json 파일)\n",
    "tokenizer.save(\"tokenizer/kojson-tokenizer.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b088c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\slm\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 306, 6, 1220, 819, 325, 3]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer/kojson-tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    bos_token=\"[BOS]\",\n",
    "    eos_token=\"[EOS]\"\n",
    ")\n",
    "\n",
    "print(tokenizer.encode(\"22,9는 우회로개방 상태야\", add_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59912fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, n_heads=4, n_layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = nn.Embedding(512, d_model)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model, n_heads, d_model * 4, dropout, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pos = torch.arange(x.size(1), device=x.device)\n",
    "        pos = self.pos_embedding(pos)[None, :, :]\n",
    "        x = self.embedding(x) + pos\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.ln(x)\n",
    "        return self.head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e50b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class PromptCompletionDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=128):\n",
    "        import json\n",
    "        self.samples = []\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                text = f\"{data['prompt']} -> {data['completion']}\"\n",
    "                tokenized = tokenizer(text, padding='max_length', truncation=True, max_length=max_len)\n",
    "                self.samples.append(torch.tensor(tokenized[\"input_ids\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb100bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MiniGPT(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "dataset = PromptCompletionDataset(\"./create-dataset/dataset/train.jsonl\", tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "print(device)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch[:, :-1])\n",
    "        loss = F.cross_entropy(output.reshape(-1, tokenizer.vocab_size), batch[:, 1:].reshape(-1), ignore_index=tokenizer.pad_token_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(loader):.4f}\")\n",
    "    torch.save(model.state_dict(), f\"models/mini_gpt_epoch{epoch+1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e63836",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      2\u001b[0m test_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m22,9는 우회로개방 상태야\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m tokenizer(test_text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_text = \"22,9는 우회로개방 상태야\"\n",
    "input_ids = tokenizer(test_text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "with torch.no_grad():\n",
    "    output = model.generate(input_ids, max_new_tokens=64)\n",
    "    print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mini_gpt.pth\")\n",
    "# 로드할 때\n",
    "model.load_state_dict(torch.load(\"mini_gpt.pth\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
